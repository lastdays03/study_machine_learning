# 4주차 Session 2: 의사결정나무와 앙상블 (Tree & Ensemble)

> **목표**: 직관적인 규칙 기반의 **의사결정나무(Decision Tree)** 원리를 이해하고, 이를 여러 개 모아 강력한 성능을 내는 **앙상블(Ensemble)** 기법의 기초인 랜덤 포레스트(Random Forest)를 학습합니다.

## 1. 의사결정나무 (Decision Tree)

스무고개 놀이처럼, 데이터의 특징(Feature)에 대해 "예/아니오" 질문을 반복하며 정답을 찾아가는 모델입니다.

### 핵심 개념
1.  **노드(Node)**: 질문이 있는 곳(분기점) 또는 최종 정답이 있는 곳(리프 노드).
2.  **불순도(Impurity)**: 해당 노드에 서로 다른 데이터가 얼마나 섞여 있는가?
    *   **지니 계수(Gini Index)** 또는 **엔트로피(Entropy)**를 사용하여 계산.
    *   질문을 던져서 불순도를 **최대한 낮추는(순수하게 만드는)** 방향으로 가지를 칩니다.

### 장단점
*   **장점**: 시각화가 가능하여 **설명력(Explainability)**이 매우 좋다. (왜 그렇게 판단했는지 알 수 있음)
*   **단점**: 훈련 데이터에 너무 과하게 맞춰지는 **과적합(Overfitting)**이 매우 잘 일어난다. (가지치기 등 규제 필요)

---

## 2. 앙상블 (Ensemble) - 랜덤 포레스트 (Random Forest)

"약한 모델 여러 개를 합치면 강한 모델이 된다"는 집단 지성의 원리를 이용합니다.
**랜덤 포레스트**는 수많은 의사결정나무를 만들고, 투표를 통해 최종 결과를 결정하는 방법입니다.

### 핵심 원리: 배깅 (Bagging)
1.  **Bootstrap**: 전체 데이터에서 일부를 **랜덤하게 중복을 허용하여** 샘플링합니다. (나무마다 서로 조금씩 다른 데이터를 공부함)
2.  **Aggregating**: 각 나무들이 내놓은 예측 결과를 모아서 다수결(투표)로 정합니다.

### 왜 강력한가?
*   나무 하나하나는 과적합되기 쉽지만, 서로 다른 개성(랜덤성)을 가진 나무들의 의견을 평균 내면 과적합이 상쇄되어 **일반화 성능이 매우 좋아집니다.**
