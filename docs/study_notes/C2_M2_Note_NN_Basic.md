# Study Note: Neural Networks & Backpropagation

**Course**: Course 2 (Master Class)
**Related Plan**: [C2_M2_Plan_Deep_Learning.md](../plans/C2_M2_Plan_Deep_Learning.md)

---

## 1. ğŸ” Theory (Textbook Deep Dive)
### 1.1 Neural Network Architecture
*   **Perceptron**: ì…ë ¥ê°’ì— ê°€ì¤‘ì¹˜(Weight)ë¥¼ ê³±í•˜ê³  í¸í–¥(Bias)ì„ ë”í•´, í™œì„±í™” í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œí‚¤ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ì‹ ê²½ë§ ë‹¨ìœ„.
*   **MLP (Multi-Layer Perceptron)**: í¼ì…‰íŠ¸ë¡ ì„ ì—¬ëŸ¬ ì¸µ(Hidden Layers) ìŒ“ì•„ ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°.
    *   **Universal Approximation Theorem**: ì€ë‹‰ì¸µì´ í•˜ë‚˜ë¼ë„ ìˆëŠ” ì‹ ê²½ë§ì€ ì„¸ìƒì˜ ì–´ë–¤ í•¨ìˆ˜ë¼ë„ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ë¡ .

### 1.2 Backpropagation (ì—­ì „íŒŒ)
*   **ëª©í‘œ**: Loss(ì˜¤ì°¨)ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê° íŒŒë¼ë¯¸í„°($W$)ë¥¼ ì–¼ë§ˆë‚˜ ìˆ˜ì •í•´ì•¼ í•˜ëŠ”ì§€(Gradient)ë¥¼ êµ¬í•˜ëŠ” ê²ƒ.
*   **Chain Rule (ì—°ì‡„ ë²•ì¹™)**:
    *   $rac{\partial Loss}{\partial x} = rac{\partial Loss}{\partial y} \cdot rac{\partial y}{\partial x}$
    *   ì¶œë ¥ì¸µì˜ ì˜¤ì°¨ë¥¼ ì…ë ¥ì¸µ ë°©í–¥ìœ¼ë¡œ ë¯¸ë¶„ê°’ì„ ì „ë‹¬í•˜ë©° ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

### 1.3 Optimization
*   **GD (Gradient Descent)**: ì‚°ì„ ë‚´ë ¤ê°€ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•. ì „ ë°ì´í„°ë¥¼ ë‹¤ ì¨ì„œ ëŠë¦¼.
*   **SGD (Stochastic GD)**: ëœë¤í•˜ê²Œ ì¼ë¶€ë§Œ ë³´ê³  ë‚´ë ¤ê°. ë¹ ë¥´ì§€ë§Œ ë¹„í‹€ê±°ë¦¼.
*   **Adam**: ê´€ì„±(Momentum)ê³¼ ë³´í­ ì¡°ì ˆ(Adaptive LR)ì„ í•©ì¹œ, ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜.

## 2. ğŸ§ª Experiment & Insight
*   **Experiment**: `C2_M2_Exp_NN_Basic.ipynb`
*   **Insight**:
    *   **Loss Function**: íšŒê·€ëŠ” MSE, ë¶„ë¥˜ëŠ” Cross-Entropy. ëª©ì ì— ë§ëŠ” Loss ì„ íƒì´ í•™ìŠµì˜ ë°©í–¥ì„ ê²°ì •í•œë‹¤.
    *   ì´ˆê¸°í™”(Initialization)ë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ í•˜ë©´ í•™ìŠµì´ ì „í˜€ ì•ˆ ëœë‹¤. (Symmetry Breaking í•„ìš”)

## 3. ğŸ”¨ Break & Fix Log
*   **Break**: Learning Rateë¥¼ 10.0ìœ¼ë¡œ ì„¤ì •.
*   **Result**: Lossê°€ ì¤„ì–´ë“¤ì§€ ì•Šê³  ë¬´í•œëŒ€ë¡œ ë°œì‚°(Exploding)í•˜ê±°ë‚˜ ì§„ë™í•¨.
