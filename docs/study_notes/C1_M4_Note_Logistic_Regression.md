# 4주차 Session 1: 로지스틱 회귀 (Logistic Regression)

> **목표**: 회귀(Regression) 알고리즘을 어떻게 분류(Classification) 문제에 적용하는지 이해하고, **시그모이드 함수(Sigmoid Function)**의 역할과 **결정 경계(Decision Boundary)**의 개념을 파악합니다.

## 1. 회귀인데 왜 분류인가?

선형 회귀($H(x) = Wx + b$)는 결과값이 $-\infty$에서 $+\infty$까지 어떤 실수값도 나올 수 있습니다.
하지만 분류 문제(예: 이 메일이 스팸인가? 생존했는가?)는 **0 또는 1**과 같이 명확한 클래스로 답해야 합니다.

로지스틱 회귀는 선형 회귀의 결과값(Score)을 **0과 1 사이의 확률값**으로 압축하여 분류 문제를 해결합니다.

---

## 2. 시그모이드 함수 (Sigmoid Function)

선형 회귀의 결과를 0~1 사이의 확률로 변환해주는 마법의 함수입니다.

$$
\text{Sigmoid}(z) = \frac{1}{1 + e^{-z}}
$$

*   입력($z = Wx + b$)이 아주 크면 → 1에 가까워짐
*   입력($z$)이 아주 작으면 → 0에 가까워짐
*   입력($z$)이 0이면 → 0.5가 됨

---

## 3. 결정 경계 (Decision Boundary)와 임계값

모델이 내놓은 확률값을 기준으로 클래스를 나눕니다.
보통 **0.5 (50%)**를 기준(Threshold, 임계값)으로 삼습니다.

*   $P(y=1|x) \ge 0.5$ $\rightarrow$ **Class 1 (양성)**
*   $P(y=1|x) < 0.5$ $\rightarrow$ **Class 0 (음성)**

---

## 4. 로지스틱 회귀의 특징

*   **장점**:
    *   구현이 쉽고 계산 속도가 매우 빠르다.
    *   결과를 **확률**로 해석할 수 있어 활용도가 높다 (예: 생존 확률 80%).
*   **단점**:
    *   결정 경계가 기본적으로 **직선(선형)**이라 복잡한 데이터 분류에는 한계가 있다.
