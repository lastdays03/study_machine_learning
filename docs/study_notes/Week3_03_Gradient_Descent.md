# 3주차 Session 3: 경사하강법 (Gradient Descent)

> **목표**: 머신러닝이 학습하는 핵심 원리인 **경사하강법**의 개념을 이해하고, 학습률(Learning Rate)이 모델 학습에 미치는 영향을 파악합니다.

## 1. 학습(Training)이란 무엇인가?

선형 회귀에서 학습이란, 데이터에 가장 잘 맞는 직선의 방정식 $H(x) = Wx + b$를 찾기 위해, **가장 적절한 $W$(기울기, Weight)와 $b$(절편, Bias)를 찾아내는 과정**입니다.

그렇다면 "가장 적절하다"는 것은 어떻게 알 수 있을까요?
바로 **예측값과 실제값의 차이(오차)**가 가장 작은 상태를 의미합니다.

---

## 2. 손실 함수 (Loss Function)

모델의 성능이 얼마나 나쁜지를 수치로 나타낸 함수입니다. "비용 함수(Cost Function)"라고도 부릅니다.
가장 대표적으로 **평균 제곱 오차 (MSE, Mean Squared Error)**를 사용합니다.

$$
Cost(W, b) = \frac{1}{m} \sum_{i=1}^{m} (H(x^{(i)}) - y^{(i)})^2
$$

*   오차가 클수록 비용(Cost) 값이 커집니다.
*   우리의 목표는 이 **Cost 값을 최소화(Minimize)**하는 $W$와 $b$를 찾는 것입니다.

---

## 3. 경사하강법 (Gradient Descent)

Cost 값이 최소가 되는 지점을 찾아가는 방법입니다.
마치 산 정상에서 눈을 가리고 가장 낮은 계곡으로 내려가기 위해, **현재 위치에서 경사(기울기)가 가파른 쪽으로 한 발자국씩 내려가는 것**과 같습니다.

### 핵심 동작 원리
1.  임의의 $W$ 값에서 시작합니다.
2.  현재 $W$ 값에서의 **미분값(기울기, Gradient)**을 구합니다.
    *   기울기가 양수(+)면: $W$를 줄여야 함 (왼쪽으로 이동)
    *   기울기가 음수(-)면: $W$를 늘려야 함 (오른쪽으로 이동)
3.  기울기의 반대 방향으로 $W$를 조금 업데이트합니다.
4.  Cost가 최소가 될 때까지 반복합니다.

### 수식
$$
W \leftarrow W - \alpha \frac{\partial}{\partial W} Cost(W)
$$

*   $\alpha$ (Alpha): **학습률 (Learning Rate)**. 한 번에 얼마나 많이 이동할지 보폭을 결정합니다.

---

## 4. 학습률 (Learning Rate)의 중요성

학습률($\alpha$)은 사용자가 직접 설정해야 하는 중요한 하이퍼파라미터입니다.

1.  **너무 작을 때 (Small Learning Rate)**
    *   장점: 최솟값에 안정적으로 도달할 가능성이 높음.
    *   단점: 보폭이 너무 작아 학습 속도가 매우 느림 (거북이).
2.  **너무 클 때 (Large Learning Rate)**
    *   장점: 초반에 빠르게 내려갈 수 있음.
    *   단점: 보폭이 너무 커서 최솟값을 지나치거나, 심하면 발산(Divergence)하여 값을 영원히 못 찾을 수 있음 (통통 튀는 공).

---

## 5. Global Minimum vs Local Minimum

*   **Global Minimum**: 전체에서 가장 낮은 지점 (우리가 찾는 목표).
*   **Local Minimum**: 주변보다는 낮지만 전체로 보면 가장 낮지 않은 지점 (함정에 빠짐).

선형 회귀의 MSE 그래프는 밥그릇 모양(Convex)이므로, 항상 Global Minimum이 하나만 존재하여 경사하강법으로 최적의 해를 잘 찾을 수 있습니다. 하지만 복잡한 딥러닝 모델에서는 Local Minimum 문제가 발생할 수 있습니다.
