# 3주차 Session 1: 선형 회귀의 원리 (Linear Regression)

> **목표**: 머신러닝의 가장 기본이 되는 모델인 선형 회귀의 작동 원리를 수학적 수식이 아닌 **직관적인 개념**으로 이해합니다.

## 1. 선형 회귀란? (The "Best Fit" Line)
데이터들의 관계를 가장 잘 설명하는 **직선 하나**를 긋는 것입니다.

### 예시: 공부 시간과 성적
*   1시간 공부 -> 10점
*   2시간 공부 -> 20점
*   3시간 공부 -> 30점
*   이 데이터를 보고 우리는 본능적으로 **"아, 공부 시간에 10을 곱하면 성적이 되는구나"**라고 규칙을 찾습니다.
*   이 규칙이 바로 **선형 회귀 모델**입니다.

---

## 2. 핵심 구성 요소

### 1) 가설 (Hypothesis, $H(x)$)
우리가 찾으려는 직선의 방정식입니다.
$$H(x) = Wx + b$$
*   **$x$ (Input)**: 입력 데이터 (공부 시간).
*   **$y$ (Output)**: 예측값 (성적).
*   **$W$ (Weight, 가중치)**: 기울기. 입력이 출력에 미치는 영향력. (여기서는 `10`)
*   **$b$ (Bias, 편향)**: 절편. 기본 베이스 값. (공부 안 해도 기본으로 받는 0점 등)

### 2) 비용 함수 (Cost Function / Loss Function)
우리가 그은 직선이 **얼마나 엉터리인지** 계산하는 채점표입니다.
*   **오차 (Error)**: (내 예측값 - 실제 정답)
*   **평균 제곱 오차 (MSE, Mean Squared Error)**: 오차를 제곱해서 평균 낸 값.
*   이 비용(Cost)이 **0에 가까울수록** 좋은 모델입니다.

### 3) 경사하강법 (Gradient Descent) - "어떻게 학습하나?"
처음에는 $W(기울기)$를 아무거나(랜덤) 설정했다가, 비용(오차)을 줄이는 방향으로 조금씩 수정해 나가는 과정입니다.
*   비유: 산 정상에서 눈을 가리고 가장 낮은 골짜기로 조금씩 내려가는 것.
*   **경사(Gradient)**가 가파르면 많이 이동하고, 완만하면 조금 이동하여 결국 오차가 최소가 되는 지점을 찾습니다.

---

## 3. 요약 (이것만은 기억하세요)
1.  **선형 회귀**: 데이터를 관통하는 최적의 직선 찾기.
2.  **학습(Training)**: 오차(Cost)를 최소화하는 **$W$(기울기)와 $b$(절편)**를 찾아내는 과정.

---

## 📝 Practice Question
`notebooks/06_regression_concept.ipynb`를 만들 필요는 없습니다. 아래 질문에 대해 Markdown 파일이나 노트에 스스로 답해보세요.

1.  만약 $H(x) = 2x + 5$ 라는 모델이 있을 때, $x=3$ 이면 예측값은 얼마인가요?
2.  실제 정답이 10이라면, 오차(Error)는 얼마인가요?
3.  비용 함수(Cost Function) 값이 크다는 것은 모델의 성능이 좋다는 뜻일까요, 나쁘다는 뜻일까요?
